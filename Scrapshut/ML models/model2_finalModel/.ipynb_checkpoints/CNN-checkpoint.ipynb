{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lenovo/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CNN on kaggle fake news dataset \n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import os\n",
    "def load_kagglefakenews():\n",
    "  \n",
    "    df = pd.read_csv('Kaggle_FakeNews/train.csv', nrows=10000, encoding='utf8')\n",
    "    train_data = df['text'].values.tolist() \n",
    "    train_labels = df['label'].values.tolist() \n",
    "\n",
    "\n",
    "    combo = list(zip(train_data, train_labels))\n",
    "    random.shuffle(combo)\n",
    "    train_data, train_labels = zip(*combo)\n",
    "    del df\n",
    "\n",
    "    return np.asarray(train_data).tolist(), np.asarray(train_labels).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = load_kagglefakenews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "MAX_NB_WORDS=50000 #dictionary size\n",
    "MAX_SEQUENCE_LENGTH=1500 #max word length of each individual article\n",
    "EMBEDDING_DIM=300 #dimensionality of the embedding vector (50, 100, 200, 300)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "\n",
    "def tokenize_trainingdata(texts, labels):\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    pickle.dump(tokenizer, open('Models/tokenizer.p', 'wb'))\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    labels = to_categorical(labels, num_classes=len(set(labels)))\n",
    "\n",
    "    return data, labels, word_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 165537 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "X, Y, word_index = tokenize_trainingdata(train_data, train_labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data (90% train, 5% test, 5% validation)\n",
    "train_data = X[:int(len(X)*0.9)]\n",
    "train_labels = Y[:int(len(X)*0.9)]\n",
    "test_data = X[int(len(X)*0.9):int(len(X)*0.95)]\n",
    "test_labels = Y[int(len(X)*0.9):int(len(X)*0.95)]\n",
    "valid_data = X[int(len(X)*0.95):]\n",
    "valid_labels = Y[int(len(X)*0.95):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(word_index, embeddingsfile='wordEmbeddings/glove.6B.%id.txt' %EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    f = open(embeddingsfile, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        \n",
    "        values = line.split(' ') \n",
    "        word = values[0] \n",
    "        coefs = np.asarray(values[1:], dtype='float32') \n",
    "        embeddings_index[word] = coefs \n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    return embedding_layer\n",
    "    \n",
    "\n",
    "embedding_layer = load_embeddings(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, Model, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Flatten, Dense, GlobalAveragePooling1D, Dropout, LSTM, CuDNNLSTM, RNN, SimpleRNN, Conv2D, GlobalMaxPooling1D\n",
    "from keras import callbacks\n",
    "\n",
    "def baseline_model(sequence_input, embedded_sequences, classes=2):\n",
    "    x = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 3, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(256, 2, activation='relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lenovo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1500, 300)         49661400  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1496, 64)          96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 299, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 297, 128)          24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 59, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 58, 256)           65792     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              526336    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 51,424,410\n",
      "Trainable params: 1,763,010\n",
      "Non-trainable params: 49,661,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/lenovo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 9000 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "9000/9000 [==============================] - 56s 6ms/step - loss: 0.4627 - acc: 0.7639 - val_loss: 0.2819 - val_acc: 0.8860\n",
      "Epoch 2/25\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.1664 - acc: 0.9390 - val_loss: 0.1625 - val_acc: 0.9340\n",
      "Epoch 3/25\n",
      "9000/9000 [==============================] - 70s 8ms/step - loss: 0.0775 - acc: 0.9746 - val_loss: 0.1257 - val_acc: 0.9420\n",
      "Epoch 4/25\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0436 - acc: 0.9854 - val_loss: 0.1507 - val_acc: 0.9520\n",
      "Epoch 5/25\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0322 - acc: 0.9897 - val_loss: 0.1309 - val_acc: 0.9560\n",
      "Epoch 6/25\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 0.0122 - acc: 0.9966 - val_loss: 0.1492 - val_acc: 0.9540\n",
      "Epoch 7/25\n",
      "9000/9000 [==============================] - 52s 6ms/step - loss: 0.0110 - acc: 0.9960 - val_loss: 0.1498 - val_acc: 0.9620\n",
      "Epoch 8/25\n",
      "9000/9000 [==============================] - 53s 6ms/step - loss: 0.0108 - acc: 0.9950 - val_loss: 0.1277 - val_acc: 0.9580\n",
      "Epoch 9/25\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 0.0047 - acc: 0.9983 - val_loss: 0.1457 - val_acc: 0.9660\n",
      "Epoch 10/25\n",
      "9000/9000 [==============================] - 57s 6ms/step - loss: 6.0727e-04 - acc: 1.0000 - val_loss: 0.1918 - val_acc: 0.9620\n",
      "Epoch 11/25\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 1.4962e-04 - acc: 1.0000 - val_loss: 0.1957 - val_acc: 0.9600\n",
      "Epoch 12/25\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 1.1832e-04 - acc: 1.0000 - val_loss: 0.1972 - val_acc: 0.9600\n",
      "Epoch 13/25\n",
      "9000/9000 [==============================] - 57s 6ms/step - loss: 6.7764e-05 - acc: 1.0000 - val_loss: 0.1957 - val_acc: 0.9640\n",
      "Epoch 14/25\n",
      "9000/9000 [==============================] - 56s 6ms/step - loss: 5.5852e-05 - acc: 1.0000 - val_loss: 0.2059 - val_acc: 0.9640\n",
      "Epoch 15/25\n",
      "9000/9000 [==============================] - 53s 6ms/step - loss: 4.7755e-05 - acc: 1.0000 - val_loss: 0.2064 - val_acc: 0.9640\n",
      "Epoch 16/25\n",
      "9000/9000 [==============================] - 57s 6ms/step - loss: 4.2416e-05 - acc: 1.0000 - val_loss: 0.2514 - val_acc: 0.9600\n",
      "Epoch 17/25\n",
      "9000/9000 [==============================] - 59s 7ms/step - loss: 4.4877e-05 - acc: 1.0000 - val_loss: 0.2381 - val_acc: 0.9600\n",
      "Epoch 18/25\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.9186e-05 - acc: 1.0000 - val_loss: 0.2274 - val_acc: 0.9660\n",
      "Epoch 19/25\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 1.7131e-05 - acc: 1.0000 - val_loss: 0.2361 - val_acc: 0.9660\n",
      "Epoch 20/25\n",
      "9000/9000 [==============================] - 85s 9ms/step - loss: 1.1431e-05 - acc: 1.0000 - val_loss: 0.2426 - val_acc: 0.9640\n",
      "Epoch 21/25\n",
      "9000/9000 [==============================] - 54s 6ms/step - loss: 1.2408e-05 - acc: 1.0000 - val_loss: 0.2364 - val_acc: 0.9660\n",
      "Epoch 22/25\n",
      "9000/9000 [==============================] - 57s 6ms/step - loss: 7.9552e-06 - acc: 1.0000 - val_loss: 0.2514 - val_acc: 0.9660\n",
      "Epoch 23/25\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 9.6526e-06 - acc: 1.0000 - val_loss: 0.2414 - val_acc: 0.9660\n",
      "Epoch 24/25\n",
      "9000/9000 [==============================] - 94s 10ms/step - loss: 6.2345e-06 - acc: 1.0000 - val_loss: 0.2589 - val_acc: 0.9660\n",
      "Epoch 25/25\n",
      "9000/9000 [==============================] - 112s 12ms/step - loss: 8.6693e-06 - acc: 1.0000 - val_loss: 0.2536 - val_acc: 0.9660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe311553150>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH=1500\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "model = baseline_model(sequence_input, embedded_sequences, classes=2)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          validation_data=(valid_data, valid_labels),\n",
    "          epochs=25, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 2s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21101550610600578, 0.9760000109672546]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "No OpKernel was registered to support Op 'CudnnRNN' used by node cu_dnnlstm_1/CudnnRNN (defined at /home/lenovo/.local/lib/python3.7/site-packages/keras/layers/cudnn_recurrent.py:517) with these attrs: [dropout=0, seed=87654321, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", is_training=true, seed2=0]\nRegistered devices: [CPU, XLA_CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[cu_dnnlstm_1/CudnnRNN]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node cu_dnnlstm_1/CudnnRNN:\n cu_dnnlstm_1/ExpandDims_1 (defined at /home/lenovo/.local/lib/python3.7/site-packages/keras/layers/cudnn_recurrent.py:487)\t\n cu_dnnlstm_1/concat_1 (defined at /home/lenovo/.local/lib/python3.7/site-packages/keras/layers/cudnn_recurrent.py:60)\t\n cu_dnnlstm_1/transpose (defined at /home/lenovo/.local/lib/python3.7/site-packages/keras/layers/cudnn_recurrent.py:484)\t\n cu_dnnlstm_1/ExpandDims_2 (defined at /home/lenovo/.local/lib/python3.7/site-packages/keras/layers/cudnn_recurrent.py:488)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNN' used by {{node cu_dnnlstm_1/CudnnRNN}}with these attrs: [dropout=0, seed=87654321, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", is_training=true, seed2=0]\nRegistered devices: [CPU, XLA_CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[cu_dnnlstm_1/CudnnRNN]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a4e72cfaa5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1456\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1457\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1458\u001b[0m         return training_arrays.predict_loop(self, f, ins,\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_predict_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m                                                \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_updates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                                                \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict_function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                                                **kwargs)\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[1;32m   3004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3005\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_tf_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3006\u001b[0;31m         \u001b[0mv1_variable_initialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3007\u001b[0m     return tf_keras_backend.function(inputs, outputs,\n\u001b[1;32m   3008\u001b[0m                                      \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mv1_variable_initialization\u001b[0;34m()\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mv1_variable_initialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;34m'`get_session` is not available when '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             'TensorFlow is executing eagerly.')\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_keras_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 879\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    880\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lenovo/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNN' used by node cu_dnnlstm_1/CudnnRNN (defined at /home/lenovo/.local/lib/python3.7/site-packages/keras/layers/cudnn_recurrent.py:517) with these attrs: [dropout=0, seed=87654321, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", is_training=true, seed2=0]\nRegistered devices: [CPU, XLA_CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[cu_dnnlstm_1/CudnnRNN]]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node cu_dnnlstm_1/CudnnRNN:\n cu_dnnlstm_1/ExpandDims_1 (defined at /home/lenovo/.local/lib/python3.7/site-packages/keras/layers/cudnn_recurrent.py:487)\t\n cu_dnnlstm_1/concat_1 (defined at /home/lenovo/.local/lib/python3.7/site-packages/keras/layers/cudnn_recurrent.py:60)\t\n cu_dnnlstm_1/transpose (defined at /home/lenovo/.local/lib/python3.7/site-packages/keras/layers/cudnn_recurrent.py:484)\t\n cu_dnnlstm_1/ExpandDims_2 (defined at /home/lenovo/.local/lib/python3.7/site-packages/keras/layers/cudnn_recurrent.py:488)"
     ]
    }
   ],
   "source": [
    "model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(testy, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(testy, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(testy, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(testy, yhat_classes)\n",
    "print('F1 score: %f' % f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-222881793f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# serialize model to JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cnn_model.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# serialize weights to HDF5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"cnn_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lenovo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load json and create model\n",
    "json_file = open('cnn_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.9377304e-09 1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "f1 = open('scraping/article.txt', \"r\")\n",
    "text = f1.read()\n",
    "#tokenize\n",
    "tok = tokenize_text([text])\n",
    "print(model.predict(tok))  #0- real, 1- fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LSTMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(sequence_input, embedded_sequences, classes=2):\n",
    "    x = CuDNNLSTM(32,\n",
    "                  return_sequences=True)(embedded_sequences)\n",
    "    x = CuDNNLSTM(64,\n",
    "                  return_sequences=True)(x)\n",
    "    x = CuDNNLSTM(128)(x)\n",
    "    x = Dense(4096,\n",
    "              activation='relu')(x)\n",
    "    x = Dense(1024,\n",
    "              activation='relu')(x)\n",
    "    preds = Dense(classes,\n",
    "              activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting H2O\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/b2/b6f5e8c7fd1efe0f6d2640b34164b10b0e28231b89cc06542a6b3a3903a0/h2o-3.30.0.1.tar.gz (129.5MB)\n",
      "\u001b[K    0% |                                | 20kB 1.5kB/s eta 1 day, 0:07:124:08\u001b[31mException:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/cli/base_command.py\", line 143, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/commands/install.py\", line 338, in run\n",
      "    resolver.resolve(requirement_set)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/resolve.py\", line 102, in resolve\n",
      "    self._resolve_one(requirement_set, req)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/resolve.py\", line 256, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/resolve.py\", line 209, in _get_abstract_dist_for\n",
      "    self.require_hashes\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/operations/prepare.py\", line 283, in prepare_linked_requirement\n",
      "    progress_bar=self.progress_bar\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/download.py\", line 836, in unpack_url\n",
      "    progress_bar=progress_bar\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/download.py\", line 673, in unpack_http_url\n",
      "    progress_bar)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/download.py\", line 897, in _download_http_url\n",
      "    _download_url(resp, link, content_file, hashes, progress_bar)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/download.py\", line 617, in _download_url\n",
      "    hashes.check_against_chunks(downloaded_chunks)\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/utils/hashes.py\", line 48, in check_against_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/download.py\", line 585, in written_chunks\n",
      "    for chunk in chunks:\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/utils/ui.py\", line 159, in iter\n",
      "    for x in it:\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/_internal/download.py\", line 574, in resp_read\n",
      "    decode_content=False):\n",
      "  File \"/usr/share/python-wheels/urllib3-1.24.1-py2.py3-none-any.whl/urllib3/response.py\", line 494, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"/usr/share/python-wheels/urllib3-1.24.1-py2.py3-none-any.whl/urllib3/response.py\", line 459, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"/usr/lib/python2.7/contextlib.py\", line 35, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/usr/share/python-wheels/urllib3-1.24.1-py2.py3-none-any.whl/urllib3/response.py\", line 374, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n",
      "ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
