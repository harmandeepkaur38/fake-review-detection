{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "import requests \n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "accepted_scheme = ['http://', 'https://', 'ftp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_url():\n",
    "    \"\"\"\n",
    "    reads the url from url.txt\n",
    "    returns : string\n",
    "    \"\"\"\n",
    "    f1 = open(\"inputs/url.txt\", \"r\")\n",
    "    url = f1.read()\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ndtv.com/india-news/coronavirus-west-bengal-pins-covid-19-testing-delays-on-defective-kits-from-top-medical-body-icmr-2214619?pfrom=home-bigstory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(domain):\n",
    "    \"\"\"\n",
    "    args: url string\n",
    "    return:  True; if the URL is not present in the database\n",
    "             the 1st element, which is the justification for the site being in the list; otherwise\n",
    "    Used to check if the given 'domain' argument is present in the database\n",
    "    \"\"\"\n",
    "    with open('scraping/sources.csv', 'r') as csv_data:\n",
    "        fields = csv.reader(csv_data,delimiter=\"\\t\")\n",
    "        # Reads the column titles in the field\n",
    "        for row in fields:\n",
    "            # Reads the value in each row, into a list for columns\n",
    "            if domain == row[0]:\n",
    "#                 print(row[1])\n",
    "                pr = str(row[1])\n",
    "                if(row[2]):\n",
    "                    pr += \"   \"+ row[2] \n",
    "                if(row[3]):\n",
    "                    pr += \"   type  \"+ row[3]\n",
    "                if(row[4]):\n",
    "                    pr += \"   problem: \"+ row[4]\n",
    "                print(str(url)+\" reported as \"+ pr +'\\n')\n",
    "                display_write(str(url)+\" reported as \"+ pr +'\\n')\n",
    "                # Compares with the 0th element in the list 'row', which are the domain URLs\n",
    "                return row[1]\n",
    "    return 1\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_write(str):\n",
    "    \"\"\"\n",
    "    args: string\n",
    "    writes string to display.txt file which is to be dispalyed to user\n",
    "    \"\"\"\n",
    "    str = \"\\n\"+str+\"\\n\"\n",
    "    file = open(\"display.txt\",'a')\n",
    "    file.write(str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getStrippedLink(link):\n",
    "    \"\"\"\n",
    "    args: url string\n",
    "    Used to generate a string with only the domain\n",
    "    \"\"\"\n",
    "    parse = urlparse(link)\n",
    "    # Parse the given link as a URL\n",
    "    stripped_link = parse[1]\n",
    "    # Get the domain specific string from the list 'parse'\n",
    "    if 'www.' in link or 'http://' in link or 'https://' in link:\n",
    "        # Check for the given strings in the link and remove them \n",
    "        stripped_link = stripped_link.replace('https://','').replace('http://','').replace('www.','')\n",
    "    #print(stripped_link)\n",
    "    return stripped_link\n",
    "\n",
    "\n",
    "def openURL(link):\n",
    "    \"\"\"\n",
    "    agrs: url string\n",
    "    Create a Request object for the given link, with a user-agent specified \n",
    "    \"\"\"\n",
    "    web_link = urllib.request.Request(link, data=None, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    try:\n",
    "        handle = urllib.request.urlopen(web_link)\n",
    "        # Attempt to connect to the URL and store the HTML GET response in 'handle' variable\n",
    "    except urllib.error.HTTPError:\n",
    "        print(\"Page unavailable\")\n",
    "        exit()\n",
    "    return handle\n",
    "\n",
    "\n",
    "def fileTitleWrite(soup, link,filename):\n",
    "    \"\"\"\n",
    "    agrs: soup object, url string\n",
    "    Write the domain and title to the 'title.txt' file\n",
    "    \"\"\" \n",
    "    fo = open(filename, \"w\")\n",
    "    tit = soup.title.text\n",
    "    fo.write(link + \"\\n\")\n",
    "    tit = soup.title.text\n",
    "    fo.write(tit)\n",
    "    fo.close()\n",
    "    print(tit)\n",
    "    return tit\n",
    "\n",
    "\n",
    "\n",
    "def fileLinkWrite(soup, parse):\n",
    "    '''\n",
    "    args: soup object, parse string\n",
    "    Write the hyperlinked URLs to the 'links.txt' file\n",
    "    '''\n",
    "    fo = open(\"links.txt\", \"w\")\n",
    "    for link in soup.findAll('a'):\n",
    "        # Find all the 'a' tags\n",
    "        if 0 <= str(link.get('href')).find(str(parse[1])):\n",
    "            # Find the tags with 'href' and which do not match to given link's domain \n",
    "            continue\n",
    "        else:\n",
    "            for s in accepted_scheme:\n",
    "                if 0 <= str(link.get('href')).find(s):\n",
    "                    # If accepted connection type, write the hyperlink to the file\n",
    "                    fo.write(str(link.get('href')) + \"\\n\")\n",
    "    fo.close()\n",
    "\n",
    "def fileArticleWrite(soup, link,tit,filename):\n",
    "    # Write the domain and title to the 'title.txt' file\n",
    "    p_tags = soup.find_all('p')\n",
    "    # Get the text from each of the “p” tags and strip surrounding whitespace.\n",
    "    p_tags_text = [tag.get_text().strip() for tag in p_tags]\n",
    "    sentence_list = [sentence for sentence in p_tags_text if not '\\n' in sentence]\n",
    "    sentence_list = [sentence for sentence in sentence_list if '.' in sentence]\n",
    "    article = tit+ \" \"\n",
    "    article += ' '.join(sentence_list)\n",
    "    fo = open(filename, \"w\")\n",
    "    fo.write(article)\n",
    "    fo.close()\n",
    "    \n",
    "def parse(link):\n",
    "    \"\"\"\n",
    "    args: url string\n",
    "    Runs the entire script\n",
    "    \"\"\"\n",
    "    # Runs the entire script\n",
    "    stripped_link = getStrippedLink(link)\n",
    "    page = requests.get(link).text\n",
    "    #Parse the pase into res\n",
    "    soup = BeautifulSoup(page)\n",
    "    #Use BeautifulSoup to create a nested data structure out of the HTML file of the website\n",
    "    tit = fileTitleWrite(soup, stripped_link,\"title.txt\")\n",
    "    urls = pred(link,tit)\n",
    "    print(urls)\n",
    "    fileLinkWrite(soup, urlparse(link))\n",
    "    fileArticleWrite(soup, link,tit,\"article.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def checkDomain():\n",
    "    \"\"\"\n",
    "    Check the URL of the given website, by invoking the check function \n",
    "    \"\"\" \n",
    "    fo = open('title.txt')\n",
    "    link = fo.readline().rstrip(\"\\n\")\n",
    "    fo.close()\n",
    "    # Reading from the 'title.txt' file for the link of the site\n",
    "    ret = check(link)\n",
    "    # Store the return value of the check function\n",
    "    if ret==1:\n",
    "        return True\n",
    "    else:\n",
    "        reasons = check(link)\n",
    "        return False\n",
    "\n",
    "\n",
    "def checkLinks():\n",
    "    \"\"\"\n",
    "    Check the domain of all the hyperlinks in the given URL\n",
    "    \"\"\"\n",
    "    fo = open('links.txt')\n",
    "    for line in fo:\n",
    "        link = getStrippedLink(line)\n",
    "        if check(link):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def result(link):\n",
    "    \"\"\" \n",
    "    args: url string\n",
    "    Main function for invoking all the respective scripts\n",
    "    \"\"\"\n",
    "    \n",
    "    link = str(link)\n",
    "    parse(link)\n",
    "    results = \"\"\n",
    "    if checkDomain():\n",
    "        r = checkLinks()\n",
    "#         print(r)\n",
    "        if r:\n",
    "            results = \"The site is not a fake news site\"\n",
    "            # If the response is True, none of the hyperlinked URLs are present in the database\n",
    "        else:\n",
    "            results = \"The site has sources from a fake site\"\n",
    "            # If the response is False, one or more hyperlinked URLs are present in the database\n",
    "    else:\n",
    "        results =\"The site is a fake news site\"\n",
    "    display_write(str(results))\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_url(url):\n",
    "    end = url.find('.html')\n",
    "    print(end)\n",
    "#     url[start: end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "get_text_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = url.replace(\".\",\"\").replace(\"?\",\"\").replace(\".html\",\" \").replace(\"https://\",\" \").replace(\"-\",\" \").replace(\"www\",\" \").replace(\"hhtp://\",\" \").replace(\".com\",\" \").replace(\".in\",\" \").replace(\"?\",\"\").replace(\"/\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ndtvcomindia newscoronavirus west bengal pins covid 19 testing delays on defective kits from top medical body icmr 2214619pfrom=home bigstory\n"
     ]
    }
   ],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(link,title):\n",
    "    \"\"\" \n",
    "    args: url string, title\n",
    "    returns: related urls list \n",
    "    searches related urls from trusted sources and writes down their article body into \"articles+soruceno.txt\"\n",
    "    \"\"\"\n",
    "    urls = find_related_urls(title)\n",
    "    sourceno = 0\n",
    "    for link in urls:\n",
    "        page = requests.get(link).text\n",
    "        #Parse the pase into res\n",
    "        soup = BeautifulSoup(page)\n",
    "        titlefile = \"title\"+str(sourceno)+\".txt\"\n",
    "        articlefile = \"article\"+str(sourceno)+\".txt\"\n",
    "        stripped_link = getStrippedLink(link)\n",
    "        #Use BeautifulSoup to create a nested data structure out of the HTML file of the website\n",
    "        fileArticleWrite(soup, link,title,articlefile)\n",
    "        sourceno += 1\n",
    "    return urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_related_urls(title):\n",
    "    \"\"\"\n",
    "    biggest innovation!\n",
    "    args: title of article\n",
    "    returns: links of  most related articles from trusted sources\n",
    "    \"\"\"\n",
    "    try: \n",
    "        from googlesearch import search \n",
    "    except ImportError:  \n",
    "        print(\"No module named 'google' found\") \n",
    "    \n",
    "    print(title)\n",
    "    related_urls = []\n",
    "    # to search \n",
    "    query1 = \"ndtv: \"+ title\n",
    "    query2 = \"timesofindia: \"+title\n",
    "    query3 = \"hindustantimes: \" + title\n",
    "    print(\"related urls extracted from trusted sources\")\n",
    "    for q in search(query1, tld=\"com\", num=10, stop=1, pause=2): \n",
    "        print(q)\n",
    "        related_urls.append(q)\n",
    "    for r in search(query2, tld=\"co.in\", num=10, stop=1, pause=2): \n",
    "        print(r)\n",
    "        related_urls.append(r)\n",
    "    for s in search(query3, tld=\"com\", num=10, stop=1, pause=2): \n",
    "        print(s)\n",
    "        related_urls.append(s)\n",
    "#     print(related_urls)\n",
    "    return related_urls\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus updates\n",
      "related urls extracted from trusted sources\n",
      "https://khabar.ndtv.com/news/india/india-coronavirus-lockdown-updates-41-deaths-1409-new-cases-were-reported-in-last-24-hours-2216625\n",
      "https://timesofindia.indiatimes.com/india/coronavirus-cases-in-india-live-updates-total-number-of-corona-cases-in-india-rises-to-19984-with-640-deaths/liveblog/75283503.cms\n",
      "https://www.hindustantimes.com/topic/coronavirus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://khabar.ndtv.com/news/india/india-coronavirus-lockdown-updates-41-deaths-1409-new-cases-were-reported-in-last-24-hours-2216625',\n",
       " 'https://timesofindia.indiatimes.com/india/coronavirus-cases-in-india-live-updates-total-number-of-corona-cases-in-india-rises-to-19984-with-640-deaths/liveblog/75283503.cms',\n",
       " 'https://www.hindustantimes.com/topic/coronavirus']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_related_urls(\"Coronavirus updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus: West Bengal Pins COVID-19 Testing Delays On Defective Kits From Top Medical Body ICMR\n",
      "Coronavirus: West Bengal Pins COVID-19 Testing Delays On Defective Kits From Top Medical Body ICMR\n",
      "related urls extracted from trusted sources\n",
      "https://www.ndtv.com/india-news/coronavirus-west-bengal-pins-covid-19-testing-delays-on-defective-kits-from-top-medical-body-icmr-2214619\n",
      "https://www.ndtv.com/india-news/coronavirus-west-bengal-pins-covid-19-testing-delays-on-defective-kits-from-top-medical-body-icmr-2214619\n",
      "https://www.ndtv.com/india-news/coronavirus-west-bengal-pins-covid-19-testing-delays-on-defective-kits-from-top-medical-body-icmr-2214619\n",
      "['https://www.ndtv.com/india-news/coronavirus-west-bengal-pins-covid-19-testing-delays-on-defective-kits-from-top-medical-body-icmr-2214619', 'https://www.ndtv.com/india-news/coronavirus-west-bengal-pins-covid-19-testing-delays-on-defective-kits-from-top-medical-body-icmr-2214619', 'https://www.ndtv.com/india-news/coronavirus-west-bengal-pins-covid-19-testing-delays-on-defective-kits-from-top-medical-body-icmr-2214619']\n",
      "The site has sources from a fake site\n"
     ]
    }
   ],
   "source": [
    "result(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "def extract(url):\n",
    "    #r = requests.get('http://www.aurionpro.com/')\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html\")\n",
    "\n",
    "    title = soup.title.string\n",
    "    print('TITLE IS :', title)\n",
    "\n",
    "    meta = soup.find_all('meta')\n",
    "\n",
    "    for tag in meta:\n",
    "        if 'name' in tag.attrs.keys() and tag.attrs['name'].strip().lower() in ['description', 'keywords']:\n",
    "            print('NAME    :',tag.attrs['name'].lower())\n",
    "            print('CONTENT :',tag.attrs['content'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE IS : Economic Chaos Fuels Hunger and Strongmen - The New York Times\n",
      "NAME    : description\n",
      "CONTENT : The E.U. accused Poland and Hungary of undermining democracy — and gave them billions in aid. Iran may be executing prisoners who feared infection.\n"
     ]
    }
   ],
   "source": [
    "extract(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
