{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LSTM on kaggle fake news dataset \n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import os\n",
    "def load_kagglefakenews():\n",
    "  \n",
    "    df = pd.read_csv('Kaggle_FakeNews/train.csv', nrows=10000, encoding='utf8')\n",
    "    train_data = df['text'].values.tolist() \n",
    "    train_labels = df['label'].values.tolist() \n",
    "\n",
    "\n",
    "    combo = list(zip(train_data, train_labels))\n",
    "    random.shuffle(combo)\n",
    "    train_data, train_labels = zip(*combo)\n",
    "    del df\n",
    "\n",
    "    return np.asarray(train_data).tolist(), np.asarray(train_labels).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = load_kagglefakenews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "\n",
    "MAX_NB_WORDS=50000 #dictionary size\n",
    "MAX_SEQUENCE_LENGTH=1500 #max word length of each individual article\n",
    "EMBEDDING_DIM=300 #dimensionality of the embedding vector (50, 100, 200, 300)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "\n",
    "def tokenize_trainingdata(texts, labels):\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    pickle.dump(tokenizer, open('Models/tokenizer.p', 'wb'))\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    labels = to_categorical(labels, num_classes=len(set(labels)))\n",
    "\n",
    "    return data, labels, word_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 165537 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "X, Y, word_index = tokenize_trainingdata(train_data, train_labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data (90% train, 5% test, 5% validation)\n",
    "train_data = X[:int(len(X)*0.9)]\n",
    "train_labels = Y[:int(len(X)*0.9)]\n",
    "test_data = X[int(len(X)*0.9):int(len(X)*0.95)]\n",
    "test_labels = Y[int(len(X)*0.9):int(len(X)*0.95)]\n",
    "valid_data = X[int(len(X)*0.95):]\n",
    "valid_labels = Y[int(len(X)*0.95):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(word_index, embeddingsfile='wordEmbeddings/glove.6B.%id.txt' %EMBEDDING_DIM):\n",
    "    embeddings_index = {}\n",
    "    f = open(embeddingsfile, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        \n",
    "        values = line.split(' ') \n",
    "        word = values[0] \n",
    "        coefs = np.asarray(values[1:], dtype='float32') \n",
    "        embeddings_index[word] = coefs \n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)\n",
    "    return embedding_layer\n",
    "    \n",
    "\n",
    "embedding_layer = load_embeddings(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, Model, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Flatten, Dense, GlobalAveragePooling1D, Dropout, LSTM, CuDNNLSTM, RNN, SimpleRNN, Conv2D, GlobalMaxPooling1D\n",
    "from keras import callbacks\n",
    "\n",
    "def baseline_model(sequence_input, embedded_sequences, classes=2):\n",
    "    x = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(128, 3, activation='relu')(x)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Conv1D(256, 2, activation='relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    preds = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(sequence_input, embedded_sequences, classes=2):\n",
    "    x = LSTM(32,return_sequences=True)(embedded_sequences)\n",
    "    x = LSTM(64,return_sequences=True)(x)\n",
    "    x = LSTM(128)(x)\n",
    "    x = Dense(4096,activation='relu')(x)\n",
    "    x = Dense(1024,activation='relu')(x)\n",
    "    preds = Dense(classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1500, 300)         49661400  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1500, 32)          42624     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1500, 64)          24832     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              528384    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 54,553,434\n",
      "Trainable params: 4,892,034\n",
      "Non-trainable params: 49,661,400\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/lenovo/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 9000 samples, validate on 500 samples\n",
      "Epoch 1/3\n",
      "9000/9000 [==============================] - 318s 35ms/step - loss: 0.4854 - acc: 0.7610 - val_loss: 0.3577 - val_acc: 0.8360\n",
      "Epoch 2/3\n",
      "9000/9000 [==============================] - 325s 36ms/step - loss: 0.3097 - acc: 0.8703 - val_loss: 0.2704 - val_acc: 0.8960\n",
      "Epoch 3/3\n",
      "9000/9000 [==============================] - 574s 64ms/step - loss: 0.2538 - acc: 0.8952 - val_loss: 0.2508 - val_acc: 0.9040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f16f3e3e610>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MAX_SEQUENCE_LENGTH=1500\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "model = LSTM_model(sequence_input, embedded_sequences, classes=2)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          validation_data=(valid_data, valid_labels),\n",
    "          epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"lstm_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"lstm_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load json and create model\n",
    "json_file = open('lstm_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"lstm_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 11s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20191344308853149, 0.9139999747276306]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_classes = []\n",
    "for i in range(len(yhat_probs)):\n",
    "    if(yhat_probs[i][0]<.5):\n",
    "        yhat_classes.append(0.)\n",
    "    else:\n",
    "        yhat_classes.append(1.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yhat_classes = np.array(yhat_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_test_labels = []\n",
    "for i in range(len(test_labels)):\n",
    "    if(test_labels[i][0]==0):\n",
    "        single_test_labels.append(0.) \n",
    "    else:\n",
    "        single_test_labels.append(1.)\n",
    "single_test_labels= np.array(single_test_labels)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.914000\n",
      "Precision: 0.966245\n",
      "Recall: 0.867424\n",
      "F1 score: 0.914172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(single_test_labels, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(single_test_labels, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(single_test_labels, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(single_test_labels, yhat_classes)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.3264155 96.67359  ]]\n"
     ]
    }
   ],
   "source": [
    "#prediction on real time data\n",
    "f1 = open('scraping/article.txt', \"r\")\n",
    "text = f1.read()\n",
    "#tokenize\n",
    "tok = tokenize_text([text])\n",
    "pred = model.predict(tok) # % real %fake\n",
    "print(pred*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
